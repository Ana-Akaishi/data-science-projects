{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Results: Synthetic data vs Increse Model Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I tested the same models with two different techniques to handle imbalance, let's see which one performed better.\n",
    "\n",
    "- Synthetic data for minority class: `classification_report.csv`\n",
    "- Increase Penalty for positive class (minority): `classification_report2.csv`\n",
    "\n",
    "\n",
    "## Concerns\n",
    "Since EDA, I noticed train and test sets aren't similar and have very different behavior on their clients and their features related to risk. For instance, grade A clients have the same interest rate as risky grades (which doesn't make much sense specially when the loan value is similar).\n",
    "\n",
    "For this, I have couple of hypothesis for why is it happening:\n",
    "1. We are missing other important information when it comes to risk analysis: age, location, investment account, ocuppation.\n",
    "2. Train set with suspect data: in my EDA test set have the expected risk x rate relation we would expect in these sorts of scenarios. But **train set** doesn't seems to understand this relationship. And one explenation for it not reflect the real world is because train set data is synthetic generated by some algorithm like K-means.\n",
    "3. Error on **test set** label (y): as discusses, test set have all its label column `null`. This could by a data entry error and **all them are zero (non-default)** OR the label has 1 and 0 but someone forgot to add it.\n",
    "\n",
    "Since my dataset is not balanced, accuracy might be an unrealiable metric to look. I'll focus on macro avg, since it considers the proportion of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results sets\n",
    "synt = pd.read_csv('classification_report.csv')\n",
    "ip = pd.read_csv('classification_report2.csv')\n",
    "\n",
    "# Concat sets into one\n",
    "results = pd.concat([synt,ip], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  precision  recall  f1-score   support     Model\n",
      "0           0.0       0.94    0.50      0.65  26137.00     logit\n",
      "1           1.0       0.07    0.55      0.12   1776.00     logit\n",
      "2      accuracy       0.50    0.50      0.50      0.50     logit\n",
      "3     macro avg       0.51    0.52      0.39  27913.00     logit\n",
      "4  weighted avg       0.89    0.50      0.62  27913.00     logit\n",
      "0           0.0       0.94    0.48      0.63  26137.00  logit IP\n",
      "1           1.0       0.07    0.56      0.12   1776.00  logit IP\n",
      "2      accuracy       0.48    0.48      0.48      0.48  logit IP\n",
      "3     macro avg       0.50    0.52      0.38  27913.00  logit IP\n",
      "4  weighted avg       0.89    0.48      0.60  27913.00  logit IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.contains('logit')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideting the **macro avg** **f1-score** , the models are not so different in terms of performance. While Logistical Regression with synthetic data was 0.01 point better, both models are far from identifying default clients (the ideal would be f1 = 1 or closer to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  precision  recall  f1-score   support   Model\n",
      "5           0.0       0.94    1.00      0.97  26137.00     xgb\n",
      "6           1.0       0.05    0.00      0.00   1776.00     xgb\n",
      "7      accuracy       0.93    0.93      0.93      0.93     xgb\n",
      "8     macro avg       0.50    0.50      0.48  27913.00     xgb\n",
      "9  weighted avg       0.88    0.93      0.90  27913.00     xgb\n",
      "5           0.0       0.94    0.80      0.86  26137.00  xgb IP\n",
      "6           1.0       0.08    0.24      0.12   1776.00  xgb IP\n",
      "7      accuracy       0.76    0.76      0.76      0.76  xgb IP\n",
      "8     macro avg       0.51    0.52      0.49  27913.00  xgb IP\n",
      "9  weighted avg       0.88    0.76      0.82  27913.00  xgb IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.contains('xgb')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between XGBoosts, increase penalty was 0.01 point better than synthetic data. Compared to **Logit with Synthetic data** (macro avg f1: 0.39), **XGBoost IP** (macro avg f1: 0.49) had better results, and was able to classify both classes (default = 1, and non-default = 0) better than previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  precision  recall  f1-score   support         Model\n",
      "10           0.0       0.94    1.00      0.97  26137.00     Light GBM\n",
      "11           1.0       0.06    0.00      0.00   1776.00     Light GBM\n",
      "12      accuracy       0.94    0.94      0.94      0.94     Light GBM\n",
      "13     macro avg       0.50    0.50      0.48  27913.00     Light GBM\n",
      "14  weighted avg       0.88    0.94      0.91  27913.00     Light GBM\n",
      "10           0.0       0.94    0.65      0.77  26137.00  Light GBM IP\n",
      "11           1.0       0.07    0.41      0.12   1776.00  Light GBM IP\n",
      "12      accuracy       0.63    0.63      0.63      0.63  Light GBM IP\n",
      "13     macro avg       0.51    0.53      0.45  27913.00  Light GBM IP\n",
      "14  weighted avg       0.89    0.63      0.73  27913.00  Light GBM IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.lower().str.contains('light')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While XGBoost and **Light GBM** have a similar use of gradients on their algorithms, their techniques differ and we see the impact on the results. **Light GBM** will focus on maximizing the leaf in each decision tree. This allows to get the minimum error per branch, but has a high chance of overfitting.\n",
    "\n",
    "Between the both imbalance techniques, **synthetic data** provided a better result (macro avg f1: 0.48). It's very similar to **XGBoost IP** (macro avg f1: 0.49), however when we check light GBM other metrics we see it was unable to classify the default (y=1) clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  precision  recall  f1-score   support   Model\n",
      "15           0.0       0.94    0.30      0.46  26137.00     SVM\n",
      "16           1.0       0.07    0.73      0.12   1776.00     SVM\n",
      "17      accuracy       0.33    0.33      0.33      0.33     SVM\n",
      "18     macro avg       0.50    0.52      0.29  27913.00     SVM\n",
      "19  weighted avg       0.89    0.33      0.44  27913.00     SVM\n",
      "15           0.0       0.94    0.40      0.56  26137.00  SVM IP\n",
      "16           1.0       0.07    0.65      0.12   1776.00  SVM IP\n",
      "17      accuracy       0.42    0.42      0.42      0.42  SVM IP\n",
      "18     macro avg       0.51    0.52      0.34  27913.00  SVM IP\n",
      "19  weighted avg       0.89    0.42      0.53  27913.00  SVM IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.lower().str.contains('svm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was hoping that non-linearity would be the answer for a better model, but seems I was wrong. Non-linearity doesn't seems to be better than a simple linear regression (Logit).\n",
    "\n",
    "Our winning model still **XGBoost IP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  precision  recall  f1-score   support       Model\n",
      "20           0.0       0.94    0.02      0.04  26137.00     ANN SGD\n",
      "21           1.0       0.06    0.98      0.12   1776.00     ANN SGD\n",
      "22      accuracy       0.08    0.08      0.08      0.08     ANN SGD\n",
      "23     macro avg       0.50    0.50      0.08  27913.00     ANN SGD\n",
      "24  weighted avg       0.89    0.08      0.04  27913.00     ANN SGD\n",
      "20           0.0       0.94    0.99      0.96  26137.00  ANN SGD IP\n",
      "21           1.0       0.05    0.01      0.01   1776.00  ANN SGD IP\n",
      "22      accuracy       0.93    0.93      0.93      0.93  ANN SGD IP\n",
      "23     macro avg       0.49    0.50      0.49  27913.00  ANN SGD IP\n",
      "24  weighted avg       0.88    0.93      0.90  27913.00  ANN SGD IP \n",
      "\n",
      "     Unnamed: 0  precision  recall  f1-score   support   Model\n",
      "5           0.0       0.94    0.80      0.86  26137.00  xgb IP\n",
      "6           1.0       0.08    0.24      0.12   1776.00  xgb IP\n",
      "7      accuracy       0.76    0.76      0.76      0.76  xgb IP\n",
      "8     macro avg       0.51    0.52      0.49  27913.00  xgb IP\n",
      "9  weighted avg       0.88    0.76      0.82  27913.00  xgb IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.lower().str.contains('sgd')], '\\n')\n",
    "print(results[results['Model'].str.lower().str.contains('xgb ip')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a **ANN SGD IP** (macro avg f1: 0.49) similar to **XGBoost IP** (macro avg f1: 0.49), it's time to heck other metrics to break the tie. We move to weighted avg f1 scores, this will tell us the model performance considering the major class.\n",
    "\n",
    "Now this is tricky, we see **ANN SGD IP** (weighted avg f1: 0.90) surpasses XGBoost IP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  precision  recall  f1-score   support        Model\n",
      "25           0.0       0.94    1.00      0.97  26137.00     ANN ADAM\n",
      "26           1.0       0.00    0.00      0.00   1776.00     ANN ADAM\n",
      "27      accuracy       0.94    0.94      0.94      0.94     ANN ADAM\n",
      "28     macro avg       0.47    0.50      0.48  27913.00     ANN ADAM\n",
      "29  weighted avg       0.88    0.94      0.91  27913.00     ANN ADAM\n",
      "25           0.0       0.00    0.00      0.00  26137.00  ANN ADAM IP\n",
      "26           1.0       0.06    1.00      0.12   1776.00  ANN ADAM IP\n",
      "27      accuracy       0.06    0.06      0.06      0.06  ANN ADAM IP\n",
      "28     macro avg       0.03    0.50      0.06  27913.00  ANN ADAM IP\n",
      "29  weighted avg       0.00    0.06      0.01  27913.00  ANN ADAM IP \n",
      "\n",
      "     Unnamed: 0  precision  recall  f1-score   support   Model\n",
      "5           0.0       0.94    0.80      0.86  26137.00  xgb IP\n",
      "6           1.0       0.08    0.24      0.12   1776.00  xgb IP\n",
      "7      accuracy       0.76    0.76      0.76      0.76  xgb IP\n",
      "8     macro avg       0.51    0.52      0.49  27913.00  xgb IP\n",
      "9  weighted avg       0.88    0.76      0.82  27913.00  xgb IP\n"
     ]
    }
   ],
   "source": [
    "print(results[results['Model'].str.lower().str.contains('adam')], '\\n')\n",
    "print(results[results['Model'].str.lower().str.contains('xgb ip')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we see that changing the optimizor to the classic ADAM, didn't help performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I tested different models using two different techniques to deal with imbalanced dataset. In general, increasing penalty in models seems to have a better performance than generating synthetic data. This is quite expected, since we preserve the unsees variables behavior in our models.\n",
    "\n",
    "Another interestinng thing is the result of the models itself. While it was quite difficult to find a good model, the **ANN SGD IP** (macro avg f1: 0.49) presented the best option to predict default clients even with all the differences and weird behavior I found on EDA. It is important to notice that even though the model display the best result between the models, it's only possible to detect **default clients** in 5% of the time, which is really low.\n",
    "\n",
    "Also important to notice how picking and testing optimizors change the result of our models. While ADAM is the safe bet for most situations, SGD provided better results in this architecture and dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
