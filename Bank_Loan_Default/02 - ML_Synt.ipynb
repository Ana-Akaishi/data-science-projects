{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & Synthetic Data\n",
    "\n",
    "For this notebook, I'll use the adjusted set from feature engineering notebook on Logit, XGBoost, Light GBM, SVM and Neural Network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from imblearn.over_sampling import SMOTE    #  This is the library I'll use to create synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import merged dataset (train + test)\n",
    "default_df = pd.read_csv('default_df.csv')\n",
    "\n",
    "# Separate between features (X) and answer (y)\n",
    "x = default_df.drop(['Unnamed: 0','Loan Status'], axis=1)\n",
    "y = default_df['Loan Status']\n",
    "\n",
    "# Split dataset considering train and test must have default class\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Minority Oversampling Technique - SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature engineering showed us that default clients represent 9.23% of train set. This will make our model less precise do identify default clients, which is a huge problem since it's suppose to do that.\n",
    "\n",
    "Total classes in training set 'Loan Status'\n",
    "|Code|Number of observations| Label|\n",
    "|:-:|:------:|:--:|\n",
    "|0  |  58,209 | Non-Default Clients|\n",
    "|1  |   5,920 | Default Clients |\n",
    "\n",
    "In this notebook I'll fix the imbalanced set by generating synthetic data for default clients (minority class). This technique is known as **oversampling**, and is commonly used in cases like this.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "*[SMOTE](https://www.blog.trainindata.com/smote-in-python-a-guide-to-balanced-datasets/) will analyze the dataset and find the minorty class. After that, will start to calculate the closest neighboors (k-means) start to generate extra observations based on those distance mean.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes in training set Loan Status\n",
      "0.0    60985\n",
      "1.0    60985\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create SMOTE instance\n",
    "smote = SMOTE(random_state=101)\n",
    "\n",
    "# Apply SMOTE on my TRAINING set, already split between x_train and y_train\n",
    "x_train, y_train = smote.fit_resample(x_train,y_train)\n",
    "\n",
    "# Check classes\n",
    "print('Total classes in training set', y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74797086,  1.97373137, -1.17839743, ..., -0.08747114,\n",
       "        -0.28630688,  1.20135296],\n",
       "       [-0.2205362 , -1.54737296,  2.82020707, ..., -0.08747114,\n",
       "        -0.28630688, -0.98837057],\n",
       "       [ 0.41802934, -0.89354366,  1.60311381, ..., -0.08747114,\n",
       "        -0.28630688,  1.20135296],\n",
       "       ...,\n",
       "       [ 0.39817562,  0.54742868,  0.81968218, ..., -0.08747114,\n",
       "        -0.28630688, -0.98837057],\n",
       "       [ 1.47901709,  0.93424444, -0.86987622, ..., -0.08747114,\n",
       "        -0.28630688,  1.20135296],\n",
       "       [ 1.85910968,  0.66504332, -0.29539639, ..., -0.08747114,\n",
       "        -0.28630688, -0.98837057]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a standard scaler based on train set\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Use train set as reference\n",
    "scaler.fit_transform(x_train)\n",
    "\n",
    "# Transform test without contaminating it\n",
    "scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Create instance for Logit model\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# Fit model in my training set\n",
    "logit.fit(x_train, y_train)\n",
    "\n",
    "# Predict y_test\n",
    "logit_predict = logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      " [[13063 13074]\n",
      " [  808   968]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.50      0.65     26137\n",
      "         1.0       0.07      0.55      0.12      1776\n",
      "\n",
      "    accuracy                           0.50     27913\n",
      "   macro avg       0.51      0.52      0.39     27913\n",
      "weighted avg       0.89      0.50      0.62     27913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "logit_matrix = metrics.confusion_matrix(y_test, logit_predict)\n",
    "print('Confusion Matrix','\\n',logit_matrix, '\\n')\n",
    "\n",
    "print(metrics.classification_report(y_test, logit_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit results are **bad**. The model can predict non-default (`Loan Status` = 0) with an avarage precision (F1) of 0.12, but needing to repeat the operation (recall) about half the times (0.55)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.65</td>\n",
       "      <td>26137.0</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>27913.0</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>27913.0</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support  Model\n",
       "0.0                0.94    0.50      0.65  26137.0  logit\n",
       "1.0                0.07    0.55      0.12   1776.0  logit\n",
       "accuracy           0.50    0.50      0.50      0.5  logit\n",
       "macro avg          0.51    0.52      0.39  27913.0  logit\n",
       "weighted avg       0.89    0.50      0.62  27913.0  logit"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract classification report\n",
    "class_report = metrics.classification_report(y_test, logit_predict, output_dict=True)\n",
    "class_report = pd.DataFrame(class_report).round(2).transpose()\n",
    "class_report['Model'] = 'logit'\n",
    "class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost instance\n",
    "XGB = xgb.XGBClassifier()\n",
    "\n",
    "# Fit he model\n",
    "XGB.fit(x_train, y_train)\n",
    "\n",
    "# Pedict\n",
    "xgb_predict = XGB.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[26085    52]\n",
      " [ 1773     3]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97     26137\n",
      "         1.0       0.05      0.00      0.00      1776\n",
      "\n",
      "    accuracy                           0.93     27913\n",
      "   macro avg       0.50      0.50      0.48     27913\n",
      "weighted avg       0.88      0.93      0.90     27913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze performance\n",
    "print('Confusion matrix', '\\n',metrics.confusion_matrix(y_test, xgb_predict), '\\n')\n",
    "\n",
    "print(metrics.classification_report(y_test, xgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary class report \n",
    "temp_class_report = metrics.classification_report(y_test, xgb_predict, output_dict=True)\n",
    "temp_class_report = pd.DataFrame(temp_class_report).round(2).transpose()\n",
    "temp_class_report['Model'] = 'xgb'\n",
    "\n",
    "# Concat with main df\n",
    "class_report = pd.concat([class_report, temp_class_report], axis=0)\n",
    "\n",
    "# Display final df\n",
    "# class_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 60985, number of negative: 60985\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6749\n",
      "[LightGBM] [Info] Number of data points in the train set: 121970, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    }
   ],
   "source": [
    "# Create instance\n",
    "lgb = lgbm.LGBMClassifier()\n",
    "\n",
    "# Fit in train set\n",
    "lgb.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lgb_predict = lgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[26105    32]\n",
      " [ 1774     2]] \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97     26137\n",
      "         1.0       0.06      0.00      0.00      1776\n",
      "\n",
      "    accuracy                           0.94     27913\n",
      "   macro avg       0.50      0.50      0.48     27913\n",
      "weighted avg       0.88      0.94      0.91     27913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "print('Confusion Matrix:', '\\n',metrics.confusion_matrix(y_test,lgb_predict), '\\n')\n",
    "\n",
    "# Classification report\n",
    "print(metrics.classification_report(y_test, lgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary classification report df\n",
    "temp_class_report = metrics.classification_report(y_test, lgb_predict, output_dict=True)\n",
    "temp_class_report = pd.DataFrame(temp_class_report).round(2).transpose()\n",
    "temp_class_report['Model'] = 'Light GBM'\n",
    "\n",
    "# Concat with main report\n",
    "class_report = pd.concat([class_report, temp_class_report], axis=0)\n",
    "# class_report\n",
    "class_report.to_csv('classification_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance model\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Fit\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "# Predict\n",
    "svc_predict = svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[ 7951 18186]\n",
      " [  480  1296]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.30      0.46     26137\n",
      "         1.0       0.07      0.73      0.12      1776\n",
      "\n",
      "    accuracy                           0.33     27913\n",
      "   macro avg       0.50      0.52      0.29     27913\n",
      "weighted avg       0.89      0.33      0.44     27913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "print('Confusion matrix', '\\n', metrics.confusion_matrix(y_test, svc_predict))\n",
    "\n",
    "# Classification report\n",
    "print(metrics.classification_report(y_test, svc_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.65</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.29</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score   support      Model\n",
       "0.0                0.94    0.50      0.65  26137.00      logit\n",
       "1.0                0.07    0.55      0.12   1776.00      logit\n",
       "accuracy           0.50    0.50      0.50      0.50      logit\n",
       "macro avg          0.51    0.52      0.39  27913.00      logit\n",
       "weighted avg       0.89    0.50      0.62  27913.00      logit\n",
       "0.0                0.94    1.00      0.97  26137.00        xgb\n",
       "1.0                0.05    0.00      0.00   1776.00        xgb\n",
       "accuracy           0.93    0.93      0.93      0.93        xgb\n",
       "macro avg          0.50    0.50      0.48  27913.00        xgb\n",
       "weighted avg       0.88    0.93      0.90  27913.00        xgb\n",
       "0.0                0.94    1.00      0.97  26137.00  Light GBM\n",
       "1.0                0.06    0.00      0.00   1776.00  Light GBM\n",
       "accuracy           0.94    0.94      0.94      0.94  Light GBM\n",
       "macro avg          0.50    0.50      0.48  27913.00  Light GBM\n",
       "weighted avg       0.88    0.94      0.91  27913.00  Light GBM\n",
       "0.0                0.94    0.30      0.46  26137.00        SVM\n",
       "1.0                0.07    0.73      0.12   1776.00        SVM\n",
       "accuracy           0.33    0.33      0.33      0.33        SVM\n",
       "macro avg          0.50    0.52      0.29  27913.00        SVM\n",
       "weighted avg       0.89    0.33      0.44  27913.00        SVM"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary classification report\n",
    "temp_class_report = metrics.classification_report(y_test, svc_predict, output_dict=True)\n",
    "temp_class_report = pd.DataFrame(temp_class_report).round(2).transpose()\n",
    "temp_class_report['Model'] = 'SVM'\n",
    "\n",
    "# Merge with report df\n",
    "class_report = pd.concat([class_report, temp_class_report], axis=0)\n",
    "# class_report\n",
    "class_report.to_csv('classification_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121970, 36)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First transform df in arrays. This is the way tensorflow builds its models\n",
    "x_train_a = x_train.to_numpy()\n",
    "y_train_a = y_train.to_numpy()\n",
    "x_test_a = x_test.to_numpy()\n",
    "y_test_a = y_test.to_numpy()\n",
    "\n",
    "x_train_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural netwrok\n",
    "ann_sgd = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, input_shape=(36,), activation='tanh'),\n",
    "  tf.keras.layers.Dense(32, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.20),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - accuracy: 0.5023 - loss: 0.7117\n",
      "Epoch 2/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 865us/step - accuracy: 0.5032 - loss: 0.6947\n",
      "Epoch 3/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 838us/step - accuracy: 0.4991 - loss: 0.6945\n",
      "Epoch 4/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 874us/step - accuracy: 0.4996 - loss: 0.6941\n",
      "Epoch 5/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 871us/step - accuracy: 0.5032 - loss: 0.6937\n",
      "Epoch 6/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 865us/step - accuracy: 0.4991 - loss: 0.6937\n",
      "Epoch 7/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 878us/step - accuracy: 0.4999 - loss: 0.6935\n",
      "Epoch 8/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 874us/step - accuracy: 0.5057 - loss: 0.6934\n",
      "Epoch 9/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 844us/step - accuracy: 0.4987 - loss: 0.6935\n",
      "Epoch 10/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 876us/step - accuracy: 0.5008 - loss: 0.6934\n",
      "Epoch 11/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 873us/step - accuracy: 0.5041 - loss: 0.6933\n",
      "Epoch 12/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 937us/step - accuracy: 0.4982 - loss: 0.6934\n",
      "Epoch 13/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 875us/step - accuracy: 0.4988 - loss: 0.6933\n",
      "Epoch 14/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 865us/step - accuracy: 0.5019 - loss: 0.6933\n",
      "Epoch 15/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 881us/step - accuracy: 0.5010 - loss: 0.6933\n",
      "Epoch 16/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 873us/step - accuracy: 0.5015 - loss: 0.6932\n",
      "Epoch 17/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 865us/step - accuracy: 0.5017 - loss: 0.6932\n",
      "Epoch 18/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 872us/step - accuracy: 0.5008 - loss: 0.6933\n",
      "Epoch 19/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 866us/step - accuracy: 0.5033 - loss: 0.6932\n",
      "Epoch 20/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 874us/step - accuracy: 0.4996 - loss: 0.6932\n",
      "Epoch 21/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 863us/step - accuracy: 0.4984 - loss: 0.6933\n",
      "Epoch 22/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 866us/step - accuracy: 0.5010 - loss: 0.6932\n",
      "Epoch 23/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 876us/step - accuracy: 0.5016 - loss: 0.6932\n",
      "Epoch 24/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 867us/step - accuracy: 0.5022 - loss: 0.6932\n",
      "Epoch 25/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 865us/step - accuracy: 0.5023 - loss: 0.6932\n",
      "Epoch 26/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 878us/step - accuracy: 0.5033 - loss: 0.6932\n",
      "Epoch 27/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 890us/step - accuracy: 0.5022 - loss: 0.6932\n",
      "Epoch 28/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 885us/step - accuracy: 0.5020 - loss: 0.6932\n",
      "Epoch 29/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 874us/step - accuracy: 0.5003 - loss: 0.6932\n",
      "Epoch 30/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 852us/step - accuracy: 0.4965 - loss: 0.6933\n",
      "Epoch 31/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 864us/step - accuracy: 0.4995 - loss: 0.6933\n",
      "Epoch 32/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 873us/step - accuracy: 0.4999 - loss: 0.6933\n",
      "Epoch 33/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 855us/step - accuracy: 0.4997 - loss: 0.6932\n",
      "Epoch 34/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 850us/step - accuracy: 0.5021 - loss: 0.6932\n",
      "Epoch 35/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 846us/step - accuracy: 0.5012 - loss: 0.6932\n",
      "Epoch 36/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 860us/step - accuracy: 0.4994 - loss: 0.6933\n",
      "Epoch 37/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 867us/step - accuracy: 0.5022 - loss: 0.6931\n",
      "Epoch 38/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 857us/step - accuracy: 0.5018 - loss: 0.6932\n",
      "Epoch 39/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 864us/step - accuracy: 0.4999 - loss: 0.6932\n",
      "Epoch 40/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 882us/step - accuracy: 0.5030 - loss: 0.6932\n",
      "Epoch 41/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 866us/step - accuracy: 0.4958 - loss: 0.6934\n",
      "Epoch 42/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 873us/step - accuracy: 0.5034 - loss: 0.6931\n",
      "Epoch 43/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 891us/step - accuracy: 0.5012 - loss: 0.6932\n",
      "Epoch 44/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 867us/step - accuracy: 0.5021 - loss: 0.6931\n",
      "Epoch 45/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 876us/step - accuracy: 0.5012 - loss: 0.6932\n",
      "Epoch 46/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 880us/step - accuracy: 0.4997 - loss: 0.6932\n",
      "Epoch 47/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 875us/step - accuracy: 0.5005 - loss: 0.6932\n",
      "Epoch 48/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 872us/step - accuracy: 0.5007 - loss: 0.6932\n",
      "Epoch 49/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 870us/step - accuracy: 0.5002 - loss: 0.6932\n",
      "Epoch 50/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 868us/step - accuracy: 0.4997 - loss: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2210ff89550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and fit\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Since this is a classification problem, our loss analysis also changes from MSE to binarycrossentropy\n",
    "ann_sgd.compile(optimizer=opt, \n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Predic\n",
    "ann_sgd.fit(x_train_a, y_train_a, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 694us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using ANN, and here I'll round rthe predictions so we have 1 (defualt) and 0 (non-default) instead of float numbers\n",
    "ann_sgd_predict = (ann_sgd.predict(x_test_a) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[  124 26013]\n",
      " [    7  1769]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.00      0.01     26137\n",
      "         1.0       0.06      1.00      0.12      1776\n",
      "\n",
      "    accuracy                           0.07     27913\n",
      "   macro avg       0.51      0.50      0.06     27913\n",
      "weighted avg       0.89      0.07      0.02     27913\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "print('Confusion matrix', '\\n', metrics.confusion_matrix(y_test_a, ann_sgd_predict))\n",
    "\n",
    "# Classification report\n",
    "print(metrics.classification_report(y_test_a, ann_sgd_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary classification report\n",
    "temp_class_report = metrics.classification_report(y_test_a, ann_sgd_predict, output_dict=True)\n",
    "temp_class_report = pd.DataFrame(temp_class_report).round(2).transpose()\n",
    "temp_class_report['Model'] = 'ANN SGD'\n",
    "\n",
    "# Merge with report df\n",
    "class_report = pd.concat([class_report, temp_class_report], axis=0)\n",
    "\n",
    "# Save\n",
    "class_report.to_csv('classification_report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.5003 - loss: 0.6965\n",
      "Epoch 2/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5009 - loss: 0.6953\n",
      "Epoch 3/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5009 - loss: 0.6952\n",
      "Epoch 4/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5022 - loss: 0.6948\n",
      "Epoch 5/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4988 - loss: 0.6955\n",
      "Epoch 6/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5002 - loss: 0.6951\n",
      "Epoch 7/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5018 - loss: 0.6954\n",
      "Epoch 8/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5013 - loss: 0.6954\n",
      "Epoch 9/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4992 - loss: 0.6954\n",
      "Epoch 10/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5007 - loss: 0.6959\n",
      "Epoch 11/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5002 - loss: 0.6953\n",
      "Epoch 12/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5014 - loss: 0.6953\n",
      "Epoch 13/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4996 - loss: 0.6958\n",
      "Epoch 14/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4990 - loss: 0.6951\n",
      "Epoch 15/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4988 - loss: 0.6960\n",
      "Epoch 16/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5006 - loss: 0.6955\n",
      "Epoch 17/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4991 - loss: 0.6957\n",
      "Epoch 18/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4991 - loss: 0.6954\n",
      "Epoch 19/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4980 - loss: 0.6956\n",
      "Epoch 20/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5019 - loss: 0.6955\n",
      "Epoch 21/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5007 - loss: 0.6949\n",
      "Epoch 22/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4983 - loss: 0.6952\n",
      "Epoch 23/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4999 - loss: 0.6953\n",
      "Epoch 24/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5000 - loss: 0.6956\n",
      "Epoch 25/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5003 - loss: 0.6953\n",
      "Epoch 26/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5004 - loss: 0.6952\n",
      "Epoch 27/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5026 - loss: 0.6953\n",
      "Epoch 28/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4972 - loss: 0.6955\n",
      "Epoch 29/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4986 - loss: 0.6956\n",
      "Epoch 30/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4953 - loss: 0.6954\n",
      "Epoch 31/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4989 - loss: 0.6957\n",
      "Epoch 32/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5026 - loss: 0.6955\n",
      "Epoch 33/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5015 - loss: 0.6950\n",
      "Epoch 34/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4996 - loss: 0.6954\n",
      "Epoch 35/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4986 - loss: 0.6955\n",
      "Epoch 36/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4959 - loss: 0.6952\n",
      "Epoch 37/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5024 - loss: 0.6953\n",
      "Epoch 38/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5009 - loss: 0.6955\n",
      "Epoch 39/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4970 - loss: 0.6960\n",
      "Epoch 40/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4998 - loss: 0.6951\n",
      "Epoch 41/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5007 - loss: 0.6953\n",
      "Epoch 42/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4988 - loss: 0.6954\n",
      "Epoch 43/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4992 - loss: 0.6954\n",
      "Epoch 44/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5006 - loss: 0.6950\n",
      "Epoch 45/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4974 - loss: 0.6957\n",
      "Epoch 46/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4977 - loss: 0.6957\n",
      "Epoch 47/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4960 - loss: 0.6952\n",
      "Epoch 48/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5004 - loss: 0.6953\n",
      "Epoch 49/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.4983 - loss: 0.6956\n",
      "Epoch 50/50\n",
      "\u001b[1m3812/3812\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5007 - loss: 0.6953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x221086e0550>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using similar architecture\n",
    "ann_adam = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, input_shape=(36,), activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.20),\n",
    "  tf.keras.layers.Dense(10, activation='tanh'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and fit\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Since this is a classification problem, our loss analysis also changes from MSE to binarycrossentropy\n",
    "ann_adam.compile(optimizer=opt, \n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Fit in train sets\n",
    "ann_adam.fit(x_train_a, y_train_a, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m873/873\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step\n"
     ]
    }
   ],
   "source": [
    "# Predict using ANN, and here I'll round rthe predictions so we have 1 (defualt) and 0 (non-default) instead of float numbers\n",
    "ann_adam_predict = (ann_adam.predict(x_test_a) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[26137     0]\n",
      " [ 1776     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97     26137\n",
      "         1.0       0.00      0.00      0.00      1776\n",
      "\n",
      "    accuracy                           0.94     27913\n",
      "   macro avg       0.47      0.50      0.48     27913\n",
      "weighted avg       0.88      0.94      0.91     27913\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# See results\n",
    "print('Confusion matrix', '\\n', metrics.confusion_matrix(y_test_a, ann_adam_predict))\n",
    "\n",
    "# Classification report\n",
    "print(metrics.classification_report(y_test_a, ann_adam_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.65</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.62</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>logit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.90</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>Light GBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.29</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>ANN SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>ANN SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>ANN SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.06</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>ANN SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>ANN SGD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>26137.00</td>\n",
       "      <td>ANN ADAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1776.00</td>\n",
       "      <td>ANN ADAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>ANN ADAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.48</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>ANN ADAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.91</td>\n",
       "      <td>27913.00</td>\n",
       "      <td>ANN ADAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score   support      Model\n",
       "0.0                0.94    0.50      0.65  26137.00      logit\n",
       "1.0                0.07    0.55      0.12   1776.00      logit\n",
       "accuracy           0.50    0.50      0.50      0.50      logit\n",
       "macro avg          0.51    0.52      0.39  27913.00      logit\n",
       "weighted avg       0.89    0.50      0.62  27913.00      logit\n",
       "0.0                0.94    1.00      0.97  26137.00        xgb\n",
       "1.0                0.05    0.00      0.00   1776.00        xgb\n",
       "accuracy           0.93    0.93      0.93      0.93        xgb\n",
       "macro avg          0.50    0.50      0.48  27913.00        xgb\n",
       "weighted avg       0.88    0.93      0.90  27913.00        xgb\n",
       "0.0                0.94    1.00      0.97  26137.00  Light GBM\n",
       "1.0                0.06    0.00      0.00   1776.00  Light GBM\n",
       "accuracy           0.94    0.94      0.94      0.94  Light GBM\n",
       "macro avg          0.50    0.50      0.48  27913.00  Light GBM\n",
       "weighted avg       0.88    0.94      0.91  27913.00  Light GBM\n",
       "0.0                0.94    0.30      0.46  26137.00        SVM\n",
       "1.0                0.07    0.73      0.12   1776.00        SVM\n",
       "accuracy           0.33    0.33      0.33      0.33        SVM\n",
       "macro avg          0.50    0.52      0.29  27913.00        SVM\n",
       "weighted avg       0.89    0.33      0.44  27913.00        SVM\n",
       "0.0                0.95    0.00      0.01  26137.00    ANN SGD\n",
       "1.0                0.06    1.00      0.12   1776.00    ANN SGD\n",
       "accuracy           0.07    0.07      0.07      0.07    ANN SGD\n",
       "macro avg          0.51    0.50      0.06  27913.00    ANN SGD\n",
       "weighted avg       0.89    0.07      0.02  27913.00    ANN SGD\n",
       "0.0                0.94    1.00      0.97  26137.00   ANN ADAM\n",
       "1.0                0.00    0.00      0.00   1776.00   ANN ADAM\n",
       "accuracy           0.94    0.94      0.94      0.94   ANN ADAM\n",
       "macro avg          0.47    0.50      0.48  27913.00   ANN ADAM\n",
       "weighted avg       0.88    0.94      0.91  27913.00   ANN ADAM"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary classification report\n",
    "temp_class_report = metrics.classification_report(y_test_a, ann_adam_predict, output_dict=True)\n",
    "temp_class_report = pd.DataFrame(temp_class_report).round(2).transpose()\n",
    "temp_class_report['Model'] = 'ANN ADAM'\n",
    "\n",
    "# Merge with report df\n",
    "class_report = pd.concat([class_report, temp_class_report], axis=0)\n",
    "\n",
    "# Save\n",
    "class_report.to_csv('classification_report.csv')\n",
    "\n",
    "# Display final results\n",
    "class_report"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
