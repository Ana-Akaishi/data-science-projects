{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFF Machine Learning\n",
    "In this notebook, I'll test some machine learning (ML) algorithms on Brazilian Financial Funds. The inspiration is to create an algorithm able to forecast funds return so investor can plan and adjust their strategy.\n",
    "\n",
    "THIS IS NOT A FINANCIAL ADVISE, just an experiment to test different ML algorithms and see which one perform better. It is also possible that none will be good, which is fine! This will give enthusiasts insight to keep looking for more variables that may influence funds return.\n",
    "\n",
    "I'll test the following algorithms:\n",
    "- Linear Regression\n",
    "    1. Classic Linear Regression\n",
    "    2. Stochastic Gradient Descent (SGD)\n",
    "- Non-linear Regression: \n",
    "    1. Support Vector Machine (SVM)\n",
    "    2. XGBoost\n",
    "- Neural Networks:\n",
    "    1. Artificial Neural Networks (ANN)\n",
    "    2. Recursive Neural Networks (RNN)\n",
    "    3. Long Short Term Memory (LSTM) - *a type of layer*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn import svm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, SimpleRNN, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping data\n",
    "Before I start modeling, I need to make sure my data fits the requeriments of my algorithms. Since I'm dealing with different data scales (monetary, rates, dummy), my models will have a hard time adjusting to it all.\n",
    "\n",
    "To fix that, I'll use StandardScaler from sklearn to adjust my values based on the standard deviation. This will make sure we have a similar scale to compare my features and will be easier to models to process.\n",
    "\n",
    "Since my models will use a train, validation and test sets, it's important not influenec the test dataset. So first, I'll:\n",
    "1. Drops the rows with issues identified on my 'Cleaning Datasets' phase: all data with error on register data\n",
    "2. Split my dataset in 3: train, validation and test\n",
    "3. Create a scaler for training and valdiation: later, I'll use the **same** scaler on test set\n",
    "4. Run my models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fund DF complete:  (1243297, 30)\n",
      "Fund DF after drop:  (315769, 16)\n"
     ]
    }
   ],
   "source": [
    "# Import financial fund df\n",
    "fund_df = pd.read_csv('fund_df.csv')\n",
    "\n",
    "# Drop observations with issue on register\n",
    "print(f'Fund DF complete: ', fund_df.shape)\n",
    "fund_df = fund_df[fund_df['correct_name'] == True]\n",
    "\n",
    "# Fix year_month column to be our index since this is a timeseries\n",
    "fund_df['year_month'] = pd.to_datetime(fund_df['year_month'])\n",
    "\n",
    "# Drop COVD 19 outrbreak\n",
    "fund_df = fund_df[fund_df['year_month'] >= '2023-01-01']\n",
    "\n",
    "# Drop closed funds due to hardware limitations\n",
    "fund_df = fund_df[fund_df['SIT_EM FUNCIONAMENTO NORMAL'] >= 1]\n",
    "\n",
    "# Drops string columns used only in EDA\n",
    "fund_df = fund_df.drop(columns=['Unnamed: 0','correct_name','DENOM_SOCIAL','DT_COMPTC',\n",
    "                                'DT_REG','SIT_CANCELADA', 'SIT_FASE PRÉ-OPERACIONAL',\n",
    "                                'SIT_LIQUIDAÇÃO', 'CONDOM_Fechado','FUNDO_COTAS_N',\n",
    "                                'FUNDO_EXCLUSIVO_N','manager_name', 'issuer_name', 'big4_name'], axis=1)\n",
    "\n",
    "# Check the final shape after drops\n",
    "print(f'Fund DF after drop: ', fund_df.shape)\n",
    "\n",
    "# # Add macro dataset\n",
    "# support_df = pd.read_csv('support_df.csv')\n",
    "# support_df['year_month'] = pd.to_datetime(support_df['year_month'])\n",
    "\n",
    "# # Merge with financial fund dataframe\n",
    "# fund_df = pd.merge(fund_df, support_df, how='left',on=['year_month'])\n",
    "\n",
    "# Set year_month as index to my timeseries issue\n",
    "fund_df.set_index(['year_month','CNPJ_FUNDO'], inplace=True)\n",
    "\n",
    "# # Remove outliers\n",
    "# fund_df[(np.abs(stats.zscore(fund_df)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features (X) and independent variable (y)\n",
    "X = fund_df.drop('quota_return', axis=1)\n",
    "y = fund_df['quota_return']\n",
    "y = y.reset_index()\n",
    "\n",
    "# Adjust y dataframe to be just an array with quota return\n",
    "y = y.drop(columns=['year_month','CNPJ_FUNDO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets for traininn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Nor subset for validation and test. I'm setting it to 50% so we have an even split between validation and test\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02503269, -0.11954532, -0.0730782 , ..., -0.02204037,\n",
       "        -0.11614359, -0.05063028],\n",
       "       [-0.02502702, -0.15995166, -0.07286053, ..., -0.02203578,\n",
       "        -0.16085039, -0.0504842 ],\n",
       "       [-0.02501748, -0.16264242, -0.0730782 , ..., -0.02202776,\n",
       "        -0.16349756, -0.05063028],\n",
       "       ...,\n",
       "       [-0.0249919 , -0.15961452, -0.07302378, ..., -0.02200642,\n",
       "        -0.16047474, -0.05059376],\n",
       "       [-0.02503268, -0.1503091 , -0.0730782 , ..., -0.02204066,\n",
       "        -0.15259114, -0.05063028],\n",
       "       [-0.02486446, -0.16480699, -0.0730782 , ..., -0.02189876,\n",
       "        -0.16564023, -0.05063028]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform X_train\n",
    "scaler.fit_transform(X_train)\n",
    "\n",
    "# Only transform feature arrays (X)\n",
    "# scaler.transform(X_val)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "First, let's try a classic linear regression. I'm trying to predict quota's return (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for linear regression model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Fit the model in my training sets (X and y)\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "# Predict values for validation\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predicted Y')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot prediction vs the real result\n",
    "plt.scatter(y_test,predictions)\n",
    "plt.xlabel('Y Test')\n",
    "plt.ylabel('Predicted Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 2833.4612631354453\n",
      "MSE: 22216133853.73996\n",
      "RMSE: 149050.7760923772\n",
      "R2: 0.2063746809228999\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "print('R2:', metrics.r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\data-science-projects\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance for SGD\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "# Fit the model in my training sets (X and y)\n",
    "sgd.fit(X_train,y_train)\n",
    "\n",
    "# Predict values for validation\n",
    "predictions_sgd = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.1522118658675142e+27\n",
      "MSE: 4.559458450002044e+55\n",
      "RMSE: 6.752376211380734e+27\n",
      "R2: -1.6287719956244766e+45\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions_sgd))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions_sgd))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions_sgd)))\n",
    "print('R2:', metrics.r2_score(y_test, predictions_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Regression\n",
    "Seems like my linear model didn't perform well. Let's try non-linear models to see if I can get an improvement on my predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\data-science-projects\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model to svm but as regression\n",
    "svm_model = svm.SVR()\n",
    "\n",
    "# Fit with my dataset\n",
    "svm_model.fit(X_train,y_train)\n",
    "\n",
    "# Predict values\n",
    "predictions_svm = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1519.5690955986481\n",
      "MSE: 27995535890.49992\n",
      "RMSE: 167318.66569662787\n",
      "R2: -8.247385006376895e-05\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions_svm))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions_svm))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions_svm)))\n",
    "print('R2:', metrics.r2_score(y_test, predictions_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documentos\\My_Py_Projects\\github_DS_projects\\data-science-projects\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:35:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\objective\\regression_obj.cu:227: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model to svm but as regression\n",
    "xg_boost_model = xgb.XGBRegressor(objective = 'reg:linear')\n",
    "\n",
    "# Fit with my dataset\n",
    "xg_boost_model.fit(X_train,y_train)\n",
    "\n",
    "# Predict values\n",
    "predictions_xgb = xg_boost_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1711.616794887955\n",
      "MSE: 27835558307.486916\n",
      "RMSE: 166839.91820750487\n",
      "R2: 0.005632221698760986\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions_xgb))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions_xgb))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions_xgb)))\n",
    "print('R2:', metrics.r2_score(y_test, predictions_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorlfow works with arrays, so transform my dataframe in array\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221038, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "ann = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(128, input_shape=(13,), activation='relu'),\n",
    "  tf.keras.layers.Dense(68, activation='relu'),\n",
    "  tf.keras.layers.Dense(34, activation='relu'),\n",
    "  tf.keras.layers.Dense(18, activation='relu'),\n",
    "  tf.keras.layers.Dense(9, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 790771728908288.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 25703634944.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 41515589632.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 18595063808.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 11245010944.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 34569924608.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 11925083136.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 9072898048.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 29603741696.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 6640193024.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 13091782656.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 26547283968.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 8534227456.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 18242265088.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 10304476160.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 11246085120.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 47654649856.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 17170619392.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 23715088384.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 6900859392.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 21776314368.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 19289982976.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16579811328.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 6607248384.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 21682604032.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 22916438016.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 7646607360.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 33374904320.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6677972480.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 5486063616.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6543870976.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 10234425344.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 36123324416.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 7191666176.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 11673825280.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 18688049152.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 30355261440.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 36842926080.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 12005337088.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 24634413056.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6256291840.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 8280249856.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6819608064.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 4932308992.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16356814848.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 24625080320.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 26041702400.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 18469668864.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 5392443392.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 20856604672.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 53485850624.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 13302615040.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 22719467520.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 12471313408.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 17839446016.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6591023616.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 13554865152.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 10658434048.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 18194276352.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 49902211072.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 13618712576.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 18949050368.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 36376166400.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16320758784.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 12113384448.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 36993474560.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6297860608.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 26127783936.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 21249568768.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 15723408384.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 21119488000.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 4985540096.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 10059186176.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 30901852160.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6353470976.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 5665678336.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 7136600064.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 10449003520.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1ms/step - loss: 6911804416.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 7198701568.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6586767872.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 21083973632.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 12652587008.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 10893904896.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 4577439744.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 20881362944.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 8333751296.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 6605355008.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16091345920.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 12429205504.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 23686486016.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 15675485184.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 6168958464.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 9107621888.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 6700012544.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 17886717952.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16747255808.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 7629095936.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step - loss: 28825511936.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 20605206528.0000\n"
     ]
    }
   ],
   "source": [
    "# Compile and fit\n",
    "opt = tf.keras.optimizers.Adam(0.001)\n",
    "ann.compile(optimizer=opt, loss='mse')\n",
    "r = ann.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2961/2961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 748us/step\n"
     ]
    }
   ],
   "source": [
    "# Transform test dataframes into arrays\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Predict using ANN\n",
    "predictions_ann = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1553.87\n",
      "MSE: 27995417221.2\n",
      "RMSE: 167318.31\n",
      "R2: -0.0\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print('MAE:', round(metrics.mean_absolute_error(y_test, predictions_ann),2))\n",
    "print('MSE:', round(metrics.mean_squared_error(y_test, predictions_ann),2))\n",
    "print('RMSE:', round(np.sqrt(metrics.mean_squared_error(y_test, predictions_ann)),2))\n",
    "print('R2:', round(metrics.r2_score(y_test, predictions_ann),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 20757423849472.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 47209209856.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 28471803904.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 16583871488.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 9735028736.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 16991280128.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 6203835392.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 46813212672.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 62323875840.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 33082638336.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 56538034176.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 24112150528.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - loss: 22306363392.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 3418931456.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 17458796544.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 18225803264.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 16439319552.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 14004098048.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 12143941632.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 10605436928.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 11415676928.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 31140841472.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 50102263808.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 68134703104.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 10078142464.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 13938638848.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 8598727680.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - loss: 29298528256.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 44821708800.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 20463814656.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 16252487680.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 11472920576.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 5833419264.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 25415446528.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 37231087616.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 16750178304.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 50610688000.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 38166138880.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 75881185280.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 4517995520.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 20588640256.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 17101699072.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 11697456128.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 62696951808.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 14197282816.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 4824094720.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 5320541184.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 20503234560.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 12234735616.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 15629461504.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 46065139712.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 2375916800.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 42527309824.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 10094221312.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 11299528704.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 6168232448.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 15400440832.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 11110050816.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 22095409152.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 12830265344.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 48055660544.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 12476715008.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 8079116800.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 20977909760.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 14933024768.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 15762540544.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 7713253888.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 15667377152.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 7368239104.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5406112768.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 19814178816.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 7104683520.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 12502642688.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 15604726784.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 19715180544.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 3666587648.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 16281167872.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 6350392320.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 46173691904.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 5120114688.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5160579072.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 10131394560.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5532401152.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 20832536576.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 9880938496.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 19860856832.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 5339299328.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 21480175616.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 12757565440.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 12518943744.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 8555466752.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 21380651008.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 8105575936.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 9369897984.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 22769664000.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 32368381952.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 20845942784.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 20345356288.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2ms/step - loss: 6594733056.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step - loss: 46421622784.0000\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "# Input layer(\n",
    "i = Input(shape=(13,1))\n",
    "\n",
    "# build model using object i as input\n",
    "layer = SimpleRNN(128, activation='relu')(i)\n",
    "layer = Dense(64, activation='relu')(layer)\n",
    "layer = Dense(12, activation='relu')(layer)\n",
    "layer = Dense(6, activation='relu')(layer)\n",
    "layer = Dense(1)(layer)\n",
    "\n",
    "# Compile model\n",
    "rnn = Model(i,layer)\n",
    "rnn.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Train the model\n",
    "rnn_results = rnn.fit(X_train,y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2961/2961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "MAE: 1554.55\n",
      "MSE: 27995415044.29\n",
      "RMSE: 167318.3\n",
      "R2: -0.0\n"
     ]
    }
   ],
   "source": [
    "# Predict using RNN\n",
    "predictions_rnn = rnn.predict(X_test)\n",
    "\n",
    "# Print metrics\n",
    "print('MAE:', round(metrics.mean_absolute_error(y_test, predictions_rnn),2))\n",
    "print('MSE:', round(metrics.mean_squared_error(y_test, predictions_rnn),2))\n",
    "print('RMSE:', round(np.sqrt(metrics.mean_squared_error(y_test, predictions_rnn)),2))\n",
    "print('R2:', round(metrics.r2_score(y_test, predictions_rnn),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 6121997824.0000\n",
      "Epoch 2/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 48776765440.0000\n",
      "Epoch 3/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 8134476800.0000\n",
      "Epoch 4/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 10039878656.0000\n",
      "Epoch 5/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 22394527744.0000\n",
      "Epoch 6/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 28664934400.0000\n",
      "Epoch 7/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 6515782144.0000\n",
      "Epoch 8/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 5155446784.0000\n",
      "Epoch 9/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 14345041920.0000\n",
      "Epoch 10/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 10128363520.0000\n",
      "Epoch 11/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 13329575936.0000\n",
      "Epoch 12/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 8442863104.0000\n",
      "Epoch 13/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 25105371136.0000\n",
      "Epoch 14/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 23767775232.0000\n",
      "Epoch 15/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 6884443136.0000\n",
      "Epoch 16/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 2698888448.0000\n",
      "Epoch 17/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 43570282496.0000\n",
      "Epoch 18/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 34381238272.0000\n",
      "Epoch 19/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 3040377088.0000\n",
      "Epoch 20/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 15881076736.0000\n",
      "Epoch 21/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 6ms/step - loss: 14943040512.0000\n",
      "Epoch 22/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 27200296960.0000\n",
      "Epoch 23/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 5884968960.0000\n",
      "Epoch 24/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 6ms/step - loss: 17924612096.0000\n",
      "Epoch 25/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 7085373952.0000\n",
      "Epoch 26/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 16150457344.0000\n",
      "Epoch 27/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 12635968512.0000\n",
      "Epoch 28/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 20135778304.0000\n",
      "Epoch 29/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 41721618432.0000\n",
      "Epoch 30/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 34934534144.0000\n",
      "Epoch 31/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 17909827584.0000\n",
      "Epoch 32/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 12202499072.0000\n",
      "Epoch 33/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 10139292672.0000\n",
      "Epoch 34/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 18405519360.0000\n",
      "Epoch 35/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 28920231936.0000\n",
      "Epoch 36/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 4359121408.0000\n",
      "Epoch 37/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 39997771776.0000\n",
      "Epoch 38/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 13922151424.0000\n",
      "Epoch 39/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 6995083264.0000\n",
      "Epoch 40/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 10588032000.0000\n",
      "Epoch 41/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 12943788032.0000\n",
      "Epoch 42/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 15305959424.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 5505698816.0000\n",
      "Epoch 44/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 15268393984.0000\n",
      "Epoch 45/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 7375597056.0000\n",
      "Epoch 46/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 4629756416.0000\n",
      "Epoch 47/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 9438354432.0000\n",
      "Epoch 48/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 7505547264.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 12772990976.0000\n",
      "Epoch 50/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 10197550080.0000\n",
      "Epoch 51/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 49860055040.0000\n",
      "Epoch 52/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 32115859456.0000\n",
      "Epoch 53/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 26454323200.0000\n",
      "Epoch 54/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 11711393792.0000\n",
      "Epoch 55/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 40823971840.0000\n",
      "Epoch 56/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 15054241792.0000\n",
      "Epoch 57/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 4985950208.0000\n",
      "Epoch 58/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 14249375744.0000\n",
      "Epoch 59/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 7519984128.0000\n",
      "Epoch 60/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 18206976000.0000\n",
      "Epoch 61/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 11959603200.0000\n",
      "Epoch 62/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 12397092864.0000\n",
      "Epoch 63/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 12952941568.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 5ms/step - loss: 48564011008.0000\n",
      "Epoch 65/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 12293725184.0000\n",
      "Epoch 66/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 49357090816.0000\n",
      "Epoch 67/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 15426745344.0000\n",
      "Epoch 68/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 36874981376.0000\n",
      "Epoch 69/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 13625582592.0000\n",
      "Epoch 70/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 26811273216.0000\n",
      "Epoch 71/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 10165151744.0000\n",
      "Epoch 72/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 62564487168.0000\n",
      "Epoch 73/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 16936090624.0000\n",
      "Epoch 74/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 5718916608.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 15017303040.0000\n",
      "Epoch 76/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 17173924864.0000\n",
      "Epoch 77/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 20154079232.0000\n",
      "Epoch 78/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 26541877248.0000\n",
      "Epoch 79/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 16913892352.0000\n",
      "Epoch 80/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 9406709760.0000\n",
      "Epoch 81/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 19556210688.0000\n",
      "Epoch 82/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 12070043648.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 11237238784.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 17147705344.0000\n",
      "Epoch 85/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5ms/step - loss: 2885242624.0000\n",
      "Epoch 86/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 12414966784.0000\n",
      "Epoch 87/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 4285434112.0000\n",
      "Epoch 88/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 21103566848.0000\n",
      "Epoch 89/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 23070398464.0000\n",
      "Epoch 90/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 23482929152.0000\n",
      "Epoch 91/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 3557056512.0000\n",
      "Epoch 92/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 8929530880.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 13663163392.0000\n",
      "Epoch 94/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 9055653888.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 13595508736.0000\n",
      "Epoch 96/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 12118811648.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 13317919744.0000\n",
      "Epoch 98/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 8996722688.0000\n",
      "Epoch 99/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 6ms/step - loss: 12800704512.0000\n",
      "Epoch 100/100\n",
      "\u001b[1m6908/6908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6ms/step - loss: 23855908864.0000\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "layer = LSTM(128)(i)\n",
    "layer = Dense(64)(layer)\n",
    "layer = Dense(12)(layer)\n",
    "layer = Dense(6)(layer)\n",
    "layer = Dense(1)(layer)\n",
    "\n",
    "# Compile model\n",
    "lstm = Model(i,layer)\n",
    "lstm.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Fit LSTM\n",
    "lstm_result = lstm.fit(X_train,y_train, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2961/2961\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step\n",
      "MAE: 3459.91\n",
      "MSE: 27860304221.5\n",
      "RMSE: 166914.06\n",
      "R2: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Predict using LSTM\n",
    "predictions_lstm = lstm.predict(X_test)\n",
    "\n",
    "# Print metrics\n",
    "print('MAE:', round(metrics.mean_absolute_error(y_test, predictions_lstm),2))\n",
    "print('MSE:', round(metrics.mean_squared_error(y_test, predictions_lstm),2))\n",
    "print('RMSE:', round(np.sqrt(metrics.mean_squared_error(y_test, predictions_lstm)),2))\n",
    "print('R2:', round(metrics.r2_score(y_test, predictions_lstm),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
